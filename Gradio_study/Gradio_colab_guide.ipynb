{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOVbsHLKlvGodeB7j/6Hl9z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arekarnarayan/AIModels/blob/dev1/Gradio_colab_guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio -qqq\n",
        "!pip install \"transformers>=4.45.0\" -qqq"
      ],
      "metadata": {
        "id": "ZQo2twI5mJWN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "HUGGINGFACE_TOKEN = getpass(\"Enter your Hugging Face token:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcaQtdyzmK1a",
        "outputId": "05b385a0-6933-4080-d9f9-39a2e93f24b2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Hugging Face token:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "sec_key = userdata.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = sec_key"
      ],
      "metadata": {
        "id": "L1b8xyk4o7wv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"  # You can switch to 3B if needed\n",
        "text_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    token=HUGGINGFACE_TOKEN\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIuF-ZkypJed",
        "outputId": "3a3eecae-0fdf-4ab8-f74f-1bd6145d54f0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline\n",
        "# import torch\n",
        "\n",
        "# model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"  # You can switch to 3B if needed\n",
        "# text_pipeline = pipeline(\n",
        "#     \"text-generation\",\n",
        "#     model=model_id,\n",
        "#     torch_dtype=torch.bfloat16,\n",
        "#     device_map=\"auto\",\n",
        "#     token=sec_key  # Pass the token explicitly to HuggingFaceEndpoint\n",
        "# )"
      ],
      "metadata": {
        "id": "E2QM81LCmeNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain quantum computing in simple terms.\"\n",
        "outputs = text_pipeline(prompt, max_new_tokens=150)\n",
        "response = outputs[0][\"generated_text\"]\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqdvlCawmiYO",
        "outputId": "ab48fd2b-03d7-42d3-d65f-4594a93d5d5e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explain quantum computing in simple terms. Imagine a world where information is not just a series of 0s and 1s, but a complex web of possibilities. In this world, information can exist in multiple places at once, and it can be manipulated and changed in ways that are impossible with classical computers.\n",
            "\n",
            "Here's an analogy to help illustrate the concept:\n",
            "\n",
            "**Classical Computing: The Library**\n",
            "\n",
            "Imagine a library with a vast collection of books. Each book represents a piece of information, and the librarian can only access one book at a time. If you want to find a specific book, you have to physically search through the shelves, one by one. It takes time, and it's limited to the number of books you can hold in your hands at a time.\n",
            "\n",
            "**Quant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def generate_response(prompt):\n",
        "    response = text_pipeline(prompt, max_new_tokens=150)\n",
        "    return response[0][\"generated_text\"]\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=generate_response,\n",
        "    inputs=gr.Textbox(lines=3, placeholder=\"Type your question here...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Llama 3.2 Text Generator\",\n",
        "    description=\"Ask the Llama 3.2 model anything! It can generate text, summarize, and more.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "cO6lAZrztogT",
        "outputId": "b82bf8aa-4356-4d6b-a69b-a7b262b011af"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://4320e612610cd49f68.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4320e612610cd49f68.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}