{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNRIbe9vDXPUPUjMR8YTpNL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arekarnarayan/AIModels/blob/dev1/Simple_chatprompt_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7BEHEEQfNrP"
      },
      "outputs": [],
      "source": [
        "## Libraries Required\n",
        "!pip install langchain-huggingface\n",
        "## For API Calls\n",
        "!pip install huggingface_hub\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install  bitsandbytes\n",
        "!pip install langchain\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Environment secret keys\n",
        "# from google.colab import userdata\n",
        "# sec_key=userdata.get(\"HF_TOKEN\")\n",
        "# print(sec_key)"
      ],
      "metadata": {
        "id": "xEjtjViiFLl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_huggingface import HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "cVJpCtLBl0mL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import userdata\n",
        "# sec_key=userdata.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "# print(sec_key)"
      ],
      "metadata": {
        "id": "N1nRyKY7l3EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "################################################################\n",
        "################################################################\n",
        "\n",
        "### Use mistralai/Mistral-7B-Instruct-v0.3 model and invoke method to generate output ###\n",
        "\n",
        "################################################################\n",
        "################################################################"
      ],
      "metadata": {
        "id": "5BXZF4PG1pHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import warnings\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "# Suppress the specific FutureWarning\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")\n",
        "\n",
        "# Get the Hugging Face token from user data\n",
        "sec_key = userdata.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "\n",
        "# Set the token as an environment variable (optional, might be needed for some setups)\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = sec_key\n",
        "\n",
        "# Instantiate the HuggingFaceEndpoint\n",
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    task=\"text-generation\",  # Specify the task for the Mistral model\n",
        "    max_length=128,\n",
        "    temperature=0.7,\n",
        "    token=sec_key  # Pass the token explicitly to HuggingFaceEndpoint\n",
        ")\n",
        "\n",
        "# Perform inference\n",
        "response = llm.invoke(\"What is machine learning\")\n",
        "print(response)\n",
        "\n",
        "response = llm.invoke(\"What is Genertaive AI\")\n",
        "print(response)\n",
        "\n",
        "question=\"Who won the Cricket World Cup in the year 2011?\"\n",
        "template = \"\"\"Question: {question}\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "#print(prompt)\n",
        "\n",
        "llm_chain=LLMChain(llm=llm,prompt=prompt)\n",
        "print(llm_chain.invoke(question))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjVggDptsRBc",
        "outputId": "c4e7876a-48dc-46b5-b1b0-5902f11c8b47"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_huggingface.llms.huggingface_endpoint:WARNING! max_length is not default parameter.\n",
            "                    max_length was transferred to model_kwargs.\n",
            "                    Please make sure that max_length is what you intended.\n",
            "WARNING:langchain_huggingface.llms.huggingface_endpoint:WARNING! token is not default parameter.\n",
            "                    token was transferred to model_kwargs.\n",
            "                    Please make sure that token is what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and what are its applications? Machine learning is a subset of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It focuses on the development of computer programs that can access data and use it to learn for themselves.\n",
            "\n",
            "Machine learning is used in a wide range of applications, including:\n",
            "\n",
            "1. Image and speech recognition: Machine learning algorithms are used to identify and classify images, such as in facial recognition systems, and to transcribe speech to text, such as in voice assistants like Siri and Alexa.\n",
            "2. Natural language processing: Machine learning is used to understand and generate human language, such as in chatbots and language translation services.\n",
            "3. Predictive analytics: Machine learning algorithms are used to make predictions based on data, such as in fraud detection, stock market prediction, and customer segmentation.\n",
            "4. Autonomous vehicles: Machine learning is used to enable autonomous vehicles to navigate and make decisions on their own.\n",
            "5. Personalized recommendations: Machine learning is used to recommend products or content to users based on their past behavior and preferences.\n",
            "6. Healthcare: Machine learning is used to analyze medical images, such as X-rays and MRIs, to aid in diagnosis and treatment.\n",
            "7. Robotics: Machine learning is used to enable robots to learn and adapt to their environment, such as in industrial automation and household robots.\n",
            "\n",
            "Overall, machine learning has the potential to revolutionize many industries by automating tasks, improving decision-making, and enabling new capabilities.\n",
            "?\n",
            "\n",
            "Generative AI is a type of artificial intelligence (AI) that can create new content, such as images, text, or music, based on patterns and data it has been trained on. It is called \"generative\" because it generates new content that is not based on pre-existing templates or rules, but rather on the learned patterns from the data it has been trained on.\n",
            "\n",
            "There are several types of generative AI, including:\n",
            "\n",
            "1. Generative adversarial networks (GANs): These are a type of AI model that consists of two neural networks that work together to generate new content. One network, the generator, creates new content, while the other network, the discriminator, tries to tell the difference between the new content and real-world data. The two networks are trained together, with the generator trying to fool the discriminator, and the discriminator trying to correctly identify real and fake content.\n",
            "2. Variational autoencoders (VAEs): These are another type of AI model that can generate new content. A VAE consists of an encoder network that maps the input data to a lower-dimensional latent space, and a decoder network that maps the latent space back to the original data space. The VAE is trained to reconstruct the original data, and it can also be used to generate new data by sampling from the latent space.\n",
            "3. Recurrent neural networks (RNNs): These are a type of AI model that can process sequential data, such as text or speech. RNNs can be used to generate new sequences of data, such as text, by predicting the next word or character in a sequence based on the previous words or characters.\n",
            "\n",
            "Generative AI has many potential applications, such as creating realistic synthetic images for training other AI models, generating personalized content for users, and creating new music or art. However, it also raises ethical and practical concerns, such as the potential for generating fake news or deepfakes, and the challenges of ensuring that the AI models are fair and unbiased.\n",
            "{'question': 'Who won the Cricket World Cup in the year 2011?', 'text': ' The Cricket World Cup is an international cricket tournament that takes place every four years. The year 2011 falls between the 1999 and 2015 tournaments. The 2011 Cricket World Cup was won by India. So, the answer is India.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "################################################################\n",
        "################################################################\n",
        "\n",
        "### Use gpt2 model and huggingface pipeline to generate output ###\n",
        "\n",
        "################################################################\n",
        "################################################################"
      ],
      "metadata": {
        "id": "miF3RWRg1ebf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import warnings\n",
        "\n",
        "# Suppress the specific FutureWarning\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")\n",
        "\n",
        "model_id=\"gpt2\"\n",
        "model=AutoModelForCausalLM.from_pretrained(model_id)\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "pipe=pipeline(\"text-generation\",model=model,tokenizer=tokenizer,max_new_tokens=256)\n",
        "hf=HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "hf.invoke(\"What is machine learning\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "hIAOrg_Px0rl",
        "outputId": "bb7f56b8-7c50-4bcb-ec6f-db9b50ae5464"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"What is machine learning, then?\\n\\nMachine learning is a type of machine learning. The basic idea is that your neural network runs a computer program to create a predictive model to account for inputs and outputs. The model is designed only to help you predict the outcome of any given scenario, and then, using it, you can use it to figure out how best to allocate resources to perform different tasks on your particular task.\\n\\nFor example, this might be the way to keep track of my day's work during the day compared with other people. Using a predictive model, your machine learns this way:\\n\\nIf your machine was working on a task, it would work the best if we started in with the highest likelihood possible, or if we started at our target level and the number of tasks we could complete was the same.\\n\\nIf a person who is playing the latest baseball or playing the upcoming game had the best chance of winning, it would try to come up with the best possible outcome.\\n\\nIf your machine was not in charge of the task, but rather worked the problem at hand, or rather was completely incompetent or completely useless, it would attempt to solve it better. Because of this, your machine can only try to figure out what would happen if it were not\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "################################################################\n",
        "################################################################\n",
        "\n",
        "### Use HuggingfacePipelines With Gpu with model = gpt2 ###\n",
        "\n",
        "################################################################\n",
        "################################################################"
      ],
      "metadata": {
        "id": "stOixOSr3Fz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Use HuggingfacePipelines With Gpu\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "gpu_llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"gpt2\",\n",
        "    task=\"text-generation\",\n",
        "    device=0,  # replace with device_map=\"auto\" to use the accelerate library.\n",
        "    pipeline_kwargs={\"max_new_tokens\": 256},\n",
        ")\n",
        "\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "chain=prompt|gpu_llm\n",
        "\n",
        "question=\"What is machine learning?\"\n",
        "chain.invoke({\"question\":question})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "1Fy_thaxyTuT",
        "outputId": "fe2217b7-5565-4f94-a508-796fe80b012e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Question: What is machine learning?\\n\\nAnswer: Let's think step by step.\\n\\nThis is the basic question. Machine learning is a subset of the basic problem. It is about understanding what kinds of things you do and how your brain knows how to do it. A lot of work has been done on this field by Richard Huxley and others and I think it's important for a lot of people. Here's what I mean by this:\\n\\nFirst is the problem we are starting to solve. We have to figure out all the possible combinations of information that your brain perceives in what ways or what kinds of things it does it. It gets a rough idea of how often information may be processed by your brain at whatever information processing speed is available; that is the speed where the learning is done and is very important in that way.\\n\\nNow lets go back and think about the other part of this. We can look at the most common types of data a person has that is available in their computer. So how many times should you have it? How many times should you be doing it? Are you trying to be smart by using some sort of predictive algorithm. Your brain sees this and does not know what types of data.\\n\\nYou can learn that data by just being aware of these kinds of situations and by using what your\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}