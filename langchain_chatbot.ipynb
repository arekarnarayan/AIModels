{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arekarnarayan/AIModels/blob/main/langchain_chatbot1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.llms import Ollama\n",
        "import streamlit as st\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "str expected, not NoneType",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m load_dotenv()\n\u001b[32m     11\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mLANGCHAIN_TRACING_V2\u001b[39m\u001b[33m\"\u001b[39m]=\u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLANGCHAIN_API_KEY\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mLANGCHAIN_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m## Prompt Template\u001b[39;00m\n\u001b[32m     16\u001b[39m prompt=ChatPromptTemplate.from_messages(\n\u001b[32m     17\u001b[39m     [\n\u001b[32m     18\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mYou are a helpful assistant. Please response to the user queries\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     19\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mQuestion:\u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m     ]\n\u001b[32m     21\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:684\u001b[39m, in \u001b[36m__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:758\u001b[39m, in \u001b[36mencode\u001b[39m\u001b[34m(value)\u001b[39m\n",
            "\u001b[31mTypeError\u001b[39m: str expected, not NoneType"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "\n",
        "## Prompt Template\n",
        "\n",
        "prompt=ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",\"You are a helpful assistant. Please response to the user queries\"),\n",
        "        (\"user\",\"Question:{question}\")\n",
        "    ]\n",
        ")\n",
        "## streamlit framework\n",
        "\n",
        "st.title('Langchain Demo With LLAMA2 API')\n",
        "input_text=st.text_input(\"Search the topic u want\")\n",
        "\n",
        "# ollama LLAma2 LLm \n",
        "llm=Ollama(model=\"llama2\")\n",
        "output_parser=StrOutputParser()\n",
        "chain=prompt|llm|output_parser\n",
        "\n",
        "if input_text:\n",
        "    st.write(chain.invoke({\"question\":input_text}))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNIbHJx4YZ8icoyCT/ZfUwq",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llms",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
