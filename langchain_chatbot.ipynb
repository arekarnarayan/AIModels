{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arekarnarayan/AIModels/blob/main/langchain_chatbot1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LInzOc4EArR4",
        "outputId": "d0156df8-8732-412f-adad-74d74cd76604"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_openai\n",
        "!pip install langchain_core\n",
        "!pip install python-dotenv\n",
        "!pip install streamlit\n",
        "!pip install langchain_community\n",
        "!pip install langserve\n",
        "!pip install fastapi\n",
        "!pip install uvicorn\n",
        "!pip install sse_starlette\n",
        "!pip install bs4\n",
        "!pip install pypdf\n",
        "!pip install chromadb\n",
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rnsu9iY-FwwO",
        "outputId": "556d0bb6-e3b4-4470-b401-378ccf430e18"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmAztaVqBkwC",
        "outputId": "49901a3e-67a4-4175-b654-14b0a448a88c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-28 06:35:41.823 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-28 06:35:41.886 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-04-28 06:35:41.887 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-28 06:35:41.889 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-28 06:35:41.889 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-28 06:35:41.890 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-28 06:35:41.891 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-28 06:35:41.892 Session state does not function when running a script without `streamlit run`\n",
            "2025-04-28 06:35:41.895 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-28 06:35:41.896 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "WARNING:langchain_huggingface.llms.huggingface_endpoint:WARNING! max_length is not default parameter.\n",
            "                    max_length was transferred to model_kwargs.\n",
            "                    Please make sure that max_length is what you intended.\n",
            "WARNING:langchain_huggingface.llms.huggingface_endpoint:WARNING! token is not default parameter.\n",
            "                    token was transferred to model_kwargs.\n",
            "                    Please make sure that token is what you intended.\n"
          ]
        }
      ],
      "source": [
        "# prompt: format following code - from langchain_openai import ChatOpenAI\n",
        "# from langchain_core.prompts import ChatPromptTemplate\n",
        "# from langchain_core.output_parsers import StrOutputParser\n",
        "# import streamlit as st\n",
        "# import os\n",
        "# from dotenv import load_dotenv\n",
        "# os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
        "# ## Langmith tracking\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "# ## Prompt Template\n",
        "# prompt=ChatPromptTemplate.from_messages(\n",
        "#     [\n",
        "#         (\"system\",\"You are a helpful assistant. Please response to the user queries\"),\n",
        "#         (\"user\",\"Question:{question}\")\n",
        "#     ]\n",
        "# )\n",
        "# ## streamlit framework\n",
        "# st.title('Langchain Demo With OPENAI API')\n",
        "# input_text=st.text_input(\"Search the topic u want\")\n",
        "# # openAI LLm\n",
        "# llm=ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "# output_parser=StrOutputParser()\n",
        "# chain=prompt|llm|output_parser\n",
        "# if input_text:\n",
        "#     st.write(chain.invoke({'question':input_text}))\n",
        "import warnings\n",
        "import os\n",
        "from google.colab import userdata\n",
        "# from dotenv import load_dotenv\n",
        "\n",
        "# # Load environment variables\n",
        "# load_dotenv()\n",
        "\n",
        "# Suppress the specific FutureWarning\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Langchain imports\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# Streamlit import\n",
        "import streamlit as st\n",
        "\n",
        "# Set environment variables for tracing (ensure keys are loaded)\n",
        "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
        "#os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "\n",
        "sec_key = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "# Set the token as an environment variable (optional, might be needed for some setups)\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = sec_key\n",
        "\n",
        "\n",
        "# Prompt Template\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant. Please respond to the user queries\"),\n",
        "        (\"user\", \"Question:{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Streamlit framework\n",
        "st.title(\"Langchain Demo With OPENAI API\")\n",
        "input_text = st.text_input(\"Search the topic you want\")\n",
        "\n",
        "# OpenAI LLM\n",
        "#llm = ChatOpenAI(model=\"gpt2\")\n",
        "# Instantiate the HuggingFaceEndpoint\n",
        "repo_id = \"meta-llama/Llama-3.2-1B\"\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    task=\"text-generation\",  # Specify the task for the Mistral model\n",
        "    max_length=128,\n",
        "    temperature=0.7,\n",
        "    token=sec_key  # Pass the token explicitly to HuggingFaceEndpoint\n",
        ")\n",
        "\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "chain = prompt | llm | output_parser\n",
        "\n",
        "if input_text:\n",
        "    st.write(chain.invoke({\"question\": input_text}))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNIbHJx4YZ8icoyCT/ZfUwq",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
